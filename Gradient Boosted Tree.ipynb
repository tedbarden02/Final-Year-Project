{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba21819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a762f184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 37)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('synthetic_data_gaussian_copula.csv')\n",
    "data = data.sort_values(by='Date/Time')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724fb44b",
   "metadata": {},
   "source": [
    "Prediction of Arsenic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef8619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'No', 'Date/Time', 'Date/time end',\n",
       "       'Altitude [m]', 'Size fraction', 'Mass v [µg/m**3]', 'Na+ [µg/m**3]',\n",
       "       '[NH4]+ [µg/m**3]', 'K+ [µg/m**3]', 'Mg2+ [µg/m**3]', 'Ca2+ [µg/m**3]',\n",
       "       'Cl- [µg/m**3]', '[NO3]- [µg/m**3]', '[SO4]2- [µg/m**3]',\n",
       "       '[C2O4]2- [µg/m**3]', 'Br- [µg/m**3]', 'C org [µg/m**3]',\n",
       "       'EC [µg/m**3]', 'TC [µg/m**3]', 'Ca [ng/m**3]', 'Ti [ng/m**3]',\n",
       "       'V [ng/m**3]', 'Cr [ng/m**3]', 'Mn [ng/m**3]', 'Fe [ng/m**3]',\n",
       "       'Ni [ng/m**3]', 'Cu [ng/m**3]', 'Zn [ng/m**3]', 'Pb [ng/m**3]',\n",
       "       'As [ng/m**3]', 'Se [ng/m**3]', 'Sr [ng/m**3]', 'Rb [ng/m**3]',\n",
       "       'Ba [ng/m**3]', 'La [ng/m**3]', 'Ce [ng/m**3]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'As [ng/m³]' \n",
    "features = [col for col in data.columns if col not in ['No', 'Date/Time', 'Date/time end', 'Altitude [m]', target]]\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be3a86",
   "metadata": {},
   "source": [
    "Gradient Boosted Tree(Xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1c1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train the Gradient Boosting model\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # Regression task with squared error loss\n",
    "    n_estimators=100,             # Number of boosting rounds (trees)\n",
    "    learning_rate=0.1,            # Step size shrinkage to prevent overfitting\n",
    "    max_depth=3,                  # Maximum depth of each tree\n",
    "    random_state=42               # For reproducibility\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca57a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters for the model are: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 3, 'regressor__n_estimators': 200}\n",
      "Evaluation for Zn [ng/m**3]:\n",
      "MAE: 0.4036473855851516\n",
      "RMSE: 0.6515366125284167\n",
      "MSE: 0.4244999574650041\n",
      "R2: 0.6654635291387763\n",
      "\n",
      "Evaluation for As [ng/m**3]:\n",
      "MAE: 0.07152986233579167\n",
      "RMSE: 0.12683739358513269\n",
      "MSE: 0.01608772441146986\n",
      "R2: -0.2179146881610341\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('synthetic_data_gaussian_copula.csv')\n",
    "\n",
    "# Define columns\n",
    "targets = ['Zn [ng/m**3]', 'As [ng/m**3]']\n",
    "drop_cols = ['No', 'Date/Time', 'Date/time end', 'Duration', 'Altitude [m]']\n",
    "categorical_features = ['Size fraction']\n",
    "numerical_features = [col for col in df.columns if col not in targets + drop_cols + categorical_features]\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [3, 4, 5,6],\n",
    "    'regressor__learning_rate': [0.05, 0.1, 0.2,0.3],\n",
    "}\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring='neg_mean_absolute_error', refit=True)\n",
    "grid_search.fit(df.drop(columns=targets + drop_cols), df[targets])\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_pred = cross_val_predict(best_model, df.drop(columns=targets + drop_cols), df[targets], cv=kf)\n",
    "\n",
    "# Evaluate\n",
    "zn_true = df['Zn [ng/m**3]']\n",
    "as_true = df['As [ng/m**3]']\n",
    "zn_pred = y_pred[:, 0]  # Zn predictions\n",
    "as_pred = y_pred[:, 1]  # As predictions\n",
    "\n",
    "print(f\"The best parameters for the model are: {grid_search.best_params_}\")\n",
    "print(\"Evaluation for Zn [ng/m**3]:\")\n",
    "print(f\"MAE: {mean_absolute_error(zn_true, zn_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(zn_true, zn_pred))}\")\n",
    "print(f\"MSE: {mean_squared_error(zn_true, zn_pred)}\")\n",
    "print(f\"R2: {r2_score(zn_true, zn_pred)}\")\n",
    "\n",
    "print(\"\\nEvaluation for As [ng/m**3]:\")\n",
    "print(f\"MAE: {mean_absolute_error(as_true, as_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(as_true, as_pred))}\")\n",
    "print(f\"MSE: {mean_squared_error(as_true, as_pred)}\")\n",
    "print(f\"R2: {r2_score(as_true, as_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93344f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AdaBoostRegressor.__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Model pipeline with MultiOutputRegressor for AdaBoost\u001b[39;00m\n\u001b[32m     30\u001b[39m base_estimator = DecisionTreeRegressor(max_depth=\u001b[32m3\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     31\u001b[39m model = Pipeline(steps=[\n\u001b[32m     32\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpreprocessor\u001b[39m\u001b[33m'\u001b[39m, preprocessor),\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mregressor\u001b[39m\u001b[33m'\u001b[39m, MultiOutputRegressor(\u001b[43mAdaBoostRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[32m     34\u001b[39m ])\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Hyperparameter grid\u001b[39;00m\n\u001b[32m     36\u001b[39m param_grid = {\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__n_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m300\u001b[39m],\n\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__max_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m,\u001b[32m6\u001b[39m],\n\u001b[32m     39\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mregressor__learning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.05\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m0.2\u001b[39m,\u001b[32m0.3\u001b[39m],\n\u001b[32m     40\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: AdaBoostRegressor.__init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Define evaluation metrics from the paper\n",
    "def nse(observed, predicted):\n",
    "    \"\"\"Nash-Sutcliffe Efficiency\"\"\"\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - np.mean(observed)) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def pcc(observed, predicted):\n",
    "    \"\"\"Pearson Correlation Coefficient\"\"\"\n",
    "    return np.corrcoef(observed, predicted)[0, 1]\n",
    "\n",
    "def mape(observed, predicted):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    observed = np.where(observed == 0, 1e-10, observed)\n",
    "    return np.mean(np.abs((observed - predicted) / observed)) * 100\n",
    "\n",
    "def pbias(observed, predicted):\n",
    "    \"\"\"Percent Bias\"\"\"\n",
    "    return np.sum(predicted - observed) / np.sum(observed) * 100\n",
    "\n",
    "# Custom NSE scorer for multi-output\n",
    "def nse_scorer(y_true, y_pred):\n",
    "    numerator = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    denominator = np.sum((y_true - np.mean(y_true, axis=0)) ** 2, axis=0)\n",
    "    nse = 1 - numerator / denominator\n",
    "    return np.mean(nse)  # Average across outputs\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('synthetic_data_gaussian_copula.csv')\n",
    "\n",
    "# Define columns\n",
    "targets = ['Zn [ng/m³]', 'As [ng/m³]']\n",
    "drop_cols = ['No', 'Date/Time', 'Date/time end', 'Duration', 'Altitude [m]']\n",
    "categorical_features = ['Size fraction']\n",
    "numerical_features = [col for col in df.columns if col not in targets + drop_cols + categorical_features]\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model pipeline with MultiOutputRegressor for AdaBoost\n",
    "base_estimator = DecisionTreeRegressor(random_state=42)\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(AdaBoostRegressor(base_estimator=base_estimator, random_state=42)))\n",
    "])\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__max_depth': [3, 4, 5,6],\n",
    "    'regressor__learning_rate': [0.05, 0.1, 0.2,0.3],\n",
    "}\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid, cv=kf, scoring='neg_mean_absolute_error', refit=True)\n",
    "grid_search.fit(df.drop(columns=targets + drop_cols), df[targets])\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validated predictions\n",
    "y_pred = cross_val_predict(best_model, df.drop(columns=targets + drop_cols), df[targets], cv=kf)\n",
    "\n",
    "# Evaluate\n",
    "zn_true = df['Zn [ng/m**3]']\n",
    "as_true = df['As [ng/m**3]']\n",
    "zn_pred = y_pred[:, 0]  # Zn predictions\n",
    "as_pred = y_pred[:, 1]  # As predictions\n",
    "\n",
    "print(f\"The best parameters for the model are: {grid_search.best_params_}\")\n",
    "print(\"Evaluation for Zn [ng/m**3]:\")\n",
    "print(f\"MAE: {mean_absolute_error(zn_true, zn_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(zn_true, zn_pred))}\")\n",
    "print(f\"MSE: {mean_squared_error(zn_true, zn_pred)}\")\n",
    "print(f\"R2: {r2_score(zn_true, zn_pred)}\")\n",
    "\n",
    "print(\"\\nEvaluation for As [ng/m**3]:\")\n",
    "print(f\"MAE: {mean_absolute_error(as_true, as_pred)}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(as_true, as_pred))}\")\n",
    "print(f\"MSE: {mean_squared_error(as_true, as_pred)}\")\n",
    "print(f\"R2: {r2_score(as_true, as_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a7e8678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard k-Fold Cross-Validation (k=10)\n",
      "Best parameters: {'regressor__estimator__estimator__max_depth': 6, 'regressor__estimator__learning_rate': 0.01, 'regressor__estimator__n_estimators': 50}\n",
      "Best scores: -0.2337 (neg_mean_absolute_error)\n",
      "CV results for other metrics:\n",
      "neg_mean_absolute_error: -0.2337 ± 0.0255\n",
      "neg_mean_squared_error: -0.2059 ± 0.0595\n",
      "r2: 0.3279 ± 0.0435\n",
      "nse: 0.3279 ± 0.0435\n",
      "\n",
      "Evaluation for Zn [ng/m³]:\n",
      "MAE: 0.4102\n",
      "RMSE: 0.6313\n",
      "MSE: 0.3985\n",
      "R2: 0.6860\n",
      "NSE: 0.6860\n",
      "PCC: 0.8314\n",
      "MAPE: 77.1489%\n",
      "PBIAS: -5.7323%\n",
      "\n",
      "Evaluation for As [ng/m³]:\n",
      "MAE: 0.0573\n",
      "RMSE: 0.1154\n",
      "MSE: 0.0133\n",
      "R2: -0.0079\n",
      "NSE: -0.0079\n",
      "PCC: 0.1010\n",
      "MAPE: 793.6263%\n",
      "PBIAS: -27.9140%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Define evaluation metrics from the paper\n",
    "def nse(observed, predicted):\n",
    "    \"\"\"Nash-Sutcliffe Efficiency\"\"\"\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - np.mean(observed)) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def pcc(observed, predicted):\n",
    "    \"\"\"Pearson Correlation Coefficient\"\"\"\n",
    "    return np.corrcoef(observed, predicted)[0, 1]\n",
    "\n",
    "def mape(observed, predicted):\n",
    "    \"\"\"Mean Absolute Percentage Error\"\"\"\n",
    "    observed = np.where(observed == 0, 1e-10, observed)\n",
    "    return np.mean(np.abs((observed - predicted) / observed)) * 100\n",
    "\n",
    "def pbias(observed, predicted):\n",
    "    \"\"\"Percent Bias\"\"\"\n",
    "    return np.sum(predicted - observed) / np.sum(observed) * 100\n",
    "\n",
    "# Custom NSE scorer for multi-output\n",
    "def nse_scorer(y_true, y_pred):\n",
    "    numerator = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    denominator = np.sum((y_true - np.mean(y_true, axis=0)) ** 2, axis=0)\n",
    "    nse = 1 - numerator / denominator\n",
    "    return np.mean(nse)  # Average across outputs\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('synthetic_data_gaussian_copula.csv')\n",
    "\n",
    "# Define columns\n",
    "targets = ['Zn [ng/m**3]', 'As [ng/m**3]']\n",
    "drop_cols = ['No', 'Date/Time', 'Date/time end', 'Duration', 'Altitude [m]']\n",
    "categorical_features = ['Size fraction']\n",
    "numerical_features = [col for col in df.columns if col not in targets + drop_cols + categorical_features]\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model pipeline with MultiOutputRegressor for AdaBoost\n",
    "decision_model = DecisionTreeRegressor(random_state=42)  # Removed fixed max_depth\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(AdaBoostRegressor(estimator=decision_model, random_state=42)))\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'regressor__estimator__n_estimators': [50, 100, 200],\n",
    "    'regressor__estimator__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'regressor__estimator__estimator__max_depth': [3, 4, 5, 6]  # Corrected to target DecisionTreeRegressor\n",
    "}\n",
    "\n",
    "# Sort data by time for consistency (though only KFold used here)\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "df = df.sort_values('Date/Time')\n",
    "\n",
    "# Prepare features and targets\n",
    "X = df.drop(columns=targets + drop_cols)\n",
    "y = df[targets]\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'neg_mean_absolute_error': 'neg_mean_absolute_error',\n",
    "    'neg_mean_squared_error': 'neg_mean_squared_error',\n",
    "    'r2': 'r2',\n",
    "    'nse': make_scorer(nse_scorer, greater_is_better=True)\n",
    "}\n",
    "\n",
    "# Standard k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search_kf = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit='neg_mean_absolute_error')\n",
    "grid_search_kf.fit(X, y)\n",
    "\n",
    "# Best model from k-fold\n",
    "best_model_kf = grid_search_kf.best_estimator_\n",
    "\n",
    "# Cross-validated predictions for k-fold\n",
    "y_pred_kf = cross_val_predict(best_model_kf, X, y, cv=kf)\n",
    "\n",
    "# Evaluate for k-fold\n",
    "zn_true = df['Zn [ng/m**3]']\n",
    "as_true = df['As [ng/m**3]']\n",
    "zn_pred_kf = y_pred_kf[:, 0]  # Zn predictions\n",
    "as_pred_kf = y_pred_kf[:, 1]  # As predictions\n",
    "\n",
    "print(\"Standard k-Fold Cross-Validation (k=10)\")\n",
    "print(f\"Best parameters: {grid_search_kf.best_params_}\")\n",
    "print(f\"Best scores: {grid_search_kf.best_score_:.4f} (neg_mean_absolute_error)\")\n",
    "print(f\"CV results for other metrics:\")\n",
    "for metric in scoring:\n",
    "    mean_score = grid_search_kf.cv_results_[f'mean_test_{metric}'][grid_search_kf.best_index_]\n",
    "    std_score = grid_search_kf.cv_results_[f'std_test_{metric}'][grid_search_kf.best_index_]\n",
    "    print(f\"{metric}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation for Zn [ng/m³]:\")\n",
    "print(f\"MAE: {mean_absolute_error(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(zn_true, zn_pred_kf)):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"R2: {r2_score(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"NSE: {nse(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"PCC: {pcc(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"MAPE: {mape(zn_true, zn_pred_kf):.4f}%\")\n",
    "print(f\"PBIAS: {pbias(zn_true, zn_pred_kf):.4f}%\")\n",
    "\n",
    "print(\"\\nEvaluation for As [ng/m³]:\")\n",
    "print(f\"MAE: {mean_absolute_error(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(as_true, as_pred_kf)):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"R2: {r2_score(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"NSE: {nse(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"PCC: {pcc(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"MAPE: {mape(as_true, as_pred_kf):.4f}%\")\n",
    "print(f\"PBIAS: {pbias(as_true, as_pred_kf):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86667453",
   "metadata": {},
   "source": [
    "Relevance Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6180c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Relevance Vector Machine classes for regression and classification.\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n",
    "from sklearn.metrics.pairwise import (\n",
    "    linear_kernel,\n",
    "    rbf_kernel,\n",
    "    polynomial_kernel\n",
    ")\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.utils.validation import check_X_y\n",
    "\n",
    "\n",
    "class BaseRVM(BaseEstimator):\n",
    "\n",
    "    \"\"\"Base Relevance Vector Machine class.\n",
    "\n",
    "    Implementation of Mike Tipping's Relevance Vector Machine using the\n",
    "    scikit-learn API. Add a posterior over weights method and a predict\n",
    "    in subclass to use for classification or regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel='rbf',\n",
    "        degree=3,\n",
    "        coef1=None,\n",
    "        coef0=0.0,\n",
    "        n_iter=3000,\n",
    "        tol=1e-3,\n",
    "        alpha=1e-6,\n",
    "        threshold_alpha=1e9,\n",
    "        beta=1.e-6,\n",
    "        beta_fixed=False,\n",
    "        bias_used=True,\n",
    "        verbose=False\n",
    "    ):\n",
    "        \"\"\"Copy params to object properties, no validation.\"\"\"\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.coef1 = coef1\n",
    "        self.coef0 = coef0\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.threshold_alpha = threshold_alpha\n",
    "        self.beta = beta\n",
    "        self.beta_fixed = beta_fixed\n",
    "        self.bias_used = bias_used\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Return parameters as a dictionary.\"\"\"\n",
    "        params = {\n",
    "            'kernel': self.kernel,\n",
    "            'degree': self.degree,\n",
    "            'coef1': self.coef1,\n",
    "            'coef0': self.coef0,\n",
    "            'n_iter': self.n_iter,\n",
    "            'tol': self.tol,\n",
    "            'alpha': self.alpha,\n",
    "            'threshold_alpha': self.threshold_alpha,\n",
    "            'beta': self.beta,\n",
    "            'beta_fixed': self.beta_fixed,\n",
    "            'bias_used': self.bias_used,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        \"\"\"Set parameters using kwargs.\"\"\"\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def _apply_kernel(self, x, y):\n",
    "        \"\"\"Apply the selected kernel function to the data.\"\"\"\n",
    "        if self.kernel == 'linear':\n",
    "            phi = linear_kernel(x, y)\n",
    "        elif self.kernel == 'rbf':\n",
    "            phi = rbf_kernel(x, y, self.coef1)\n",
    "        elif self.kernel == 'poly':\n",
    "            phi = polynomial_kernel(x, y, self.degree, self.coef1, self.coef0)\n",
    "        elif callable(self.kernel):\n",
    "            phi = self.kernel(x, y)\n",
    "            if len(phi.shape) != 2:\n",
    "                raise ValueError(\n",
    "                    \"Custom kernel function did not return 2D matrix\"\n",
    "                )\n",
    "            if phi.shape[0] != x.shape[0]:\n",
    "                raise ValueError(\n",
    "                    \"Custom kernel function did not return matrix with rows\"\n",
    "                    \" equal to number of data points.\"\"\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Kernel selection is invalid.\")\n",
    "\n",
    "        if self.bias_used:\n",
    "            phi = np.append(phi, np.ones((phi.shape[0], 1)), axis=1)\n",
    "\n",
    "        return phi\n",
    "\n",
    "    def _prune(self):\n",
    "        \"\"\"Remove basis functions based on alpha values.\"\"\"\n",
    "        keep_alpha = self.alpha_ < self.threshold_alpha\n",
    "\n",
    "        if not np.any(keep_alpha):\n",
    "            keep_alpha[0] = True\n",
    "            if self.bias_used:\n",
    "                keep_alpha[-1] = True\n",
    "\n",
    "        if self.bias_used:\n",
    "            if not keep_alpha[-1]:\n",
    "                self.bias_used = False\n",
    "            self.relevance_ = self.relevance_[keep_alpha[:-1]]\n",
    "        else:\n",
    "            self.relevance_ = self.relevance_[keep_alpha]\n",
    "\n",
    "        self.alpha_ = self.alpha_[keep_alpha]\n",
    "        self.alpha_old = self.alpha_old[keep_alpha]\n",
    "        self.gamma = self.gamma[keep_alpha]\n",
    "        self.phi = self.phi[:, keep_alpha]\n",
    "        self.sigma_ = self.sigma_[np.ix_(keep_alpha, keep_alpha)]\n",
    "        self.m_ = self.m_[keep_alpha]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the RVR to the training data.\"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.phi = self._apply_kernel(X, X)\n",
    "\n",
    "        n_basis_functions = self.phi.shape[1]\n",
    "\n",
    "        self.relevance_ = X\n",
    "        self.y = y\n",
    "\n",
    "        self.alpha_ = self.alpha * np.ones(n_basis_functions)\n",
    "        self.beta_ = self.beta\n",
    "\n",
    "        self.m_ = np.zeros(n_basis_functions)\n",
    "\n",
    "        self.alpha_old = self.alpha_\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            self._posterior()\n",
    "\n",
    "            self.gamma = 1 - self.alpha_*np.diag(self.sigma_)\n",
    "            self.alpha_ = self.gamma/(self.m_ ** 2)\n",
    "\n",
    "            if not self.beta_fixed:\n",
    "                self.beta_ = (n_samples - np.sum(self.gamma))/(\n",
    "                    np.sum((y - np.dot(self.phi, self.m_)) ** 2))\n",
    "\n",
    "            self._prune()\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"Iteration: {}\".format(i))\n",
    "                print(\"Alpha: {}\".format(self.alpha_))\n",
    "                print(\"Beta: {}\".format(self.beta_))\n",
    "                print(\"Gamma: {}\".format(self.gamma))\n",
    "                print(\"m: {}\".format(self.m_))\n",
    "                print(\"Relevance Vectors: {}\".format(self.relevance_.shape[0]))\n",
    "                print()\n",
    "\n",
    "            delta = np.amax(np.absolute(self.alpha_ - self.alpha_old))\n",
    "\n",
    "            if delta < self.tol and i > 1:\n",
    "                break\n",
    "\n",
    "            self.alpha_old = self.alpha_\n",
    "\n",
    "        if self.bias_used:\n",
    "            self.bias = self.m_[-1]\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class RVR(BaseRVM, RegressorMixin):\n",
    "\n",
    "    \"\"\"Relevance Vector Machine Regression.\n",
    "\n",
    "    Implementation of Mike Tipping's Relevance Vector Machine for regression\n",
    "    using the scikit-learn API.\n",
    "    \"\"\"\n",
    "\n",
    "    def _posterior(self):\n",
    "        \"\"\"Compute the posterior distriubtion over weights.\"\"\"\n",
    "        i_s = np.diag(self.alpha_) + self.beta_ * np.dot(self.phi.T, self.phi)\n",
    "        self.sigma_ = np.linalg.inv(i_s)\n",
    "        self.m_ = self.beta_ * np.dot(self.sigma_, np.dot(self.phi.T, self.y))\n",
    "\n",
    "    def predict(self, X, eval_MSE=False):\n",
    "        \"\"\"Evaluate the RVR model at x.\"\"\"\n",
    "        phi = self._apply_kernel(X, self.relevance_)\n",
    "\n",
    "        y = np.dot(phi, self.m_)\n",
    "\n",
    "        if eval_MSE:\n",
    "            MSE = (1/self.beta_) + np.dot(phi, np.dot(self.sigma_, phi.T))\n",
    "            return y, MSE[:, 0]\n",
    "        else:\n",
    "            return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e98259b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 223\u001b[39m\n\u001b[32m    221\u001b[39m kf = KFold(n_splits=\u001b[32m10\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m    222\u001b[39m grid_search_kf = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit=\u001b[33m'\u001b[39m\u001b[33mneg_mean_absolute_error\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43mgrid_search_kf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Get the best model and predictions for k-fold\u001b[39;00m\n\u001b[32m    226\u001b[39m best_model_kf = grid_search_kf.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:859\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    857\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    863\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\pipeline.py:661\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    656\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    657\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    658\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    659\u001b[39m             all_params=params,\n\u001b[32m    660\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\multioutput.py:278\u001b[39m, in \u001b[36m_MultiOutputEstimator.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    276\u001b[39m         routed_params.estimator.fit[\u001b[33m\"\u001b[39m\u001b[33msample_weight\u001b[39m\u001b[33m\"\u001b[39m] = sample_weight\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimators_[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mn_features_in_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_features_in_ = \u001b[38;5;28mself\u001b[39m.estimators_[\u001b[32m0\u001b[39m].n_features_in_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\utils\\parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\sklearn\\multioutput.py:67\u001b[39m, in \u001b[36m_fit_estimator\u001b[39m\u001b[34m(estimator, X, y, sample_weight, **fit_params)\u001b[39m\n\u001b[32m     65\u001b[39m     estimator.fit(X, y, sample_weight=sample_weight, **fit_params)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mRVR.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.alpha_old = \u001b[38;5;28mself\u001b[39m.alpha_\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.n_iter):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_posterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28mself\u001b[39m.gamma = \u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.alpha_ * np.diag(\u001b[38;5;28mself\u001b[39m.sigma_)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28mself\u001b[39m.alpha_ = \u001b[38;5;28mself\u001b[39m.gamma / (\u001b[38;5;28mself\u001b[39m.m_ ** \u001b[32m2\u001b[39m + \u001b[32m1e-10\u001b[39m)  \u001b[38;5;66;03m# Add epsilon to avoid division by zero\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mRVR._posterior\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_posterior\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    102\u001b[39m     i_s = np.diag(\u001b[38;5;28mself\u001b[39m.alpha_) + \u001b[38;5;28mself\u001b[39m.beta_ * np.dot(\u001b[38;5;28mself\u001b[39m.phi.T, \u001b[38;5;28mself\u001b[39m.phi)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28mself\u001b[39m.sigma_ = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_s\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use pinv for numerical stability\u001b[39;00m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m.m_ = \u001b[38;5;28mself\u001b[39m.beta_ * np.dot(\u001b[38;5;28mself\u001b[39m.sigma_, np.dot(\u001b[38;5;28mself\u001b[39m.phi.T, \u001b[38;5;28mself\u001b[39m.y))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2280\u001b[39m, in \u001b[36mpinv\u001b[39m\u001b[34m(a, rcond, hermitian, rtol)\u001b[39m\n\u001b[32m   2278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap(res)\n\u001b[32m   2279\u001b[39m a = a.conjugate()\n\u001b[32m-> \u001b[39m\u001b[32m2280\u001b[39m u, s, vt = \u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhermitian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2282\u001b[39m \u001b[38;5;66;03m# discard small singular values\u001b[39;00m\n\u001b[32m   2283\u001b[39m cutoff = rcond[..., newaxis] * amax(s, axis=-\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tom\\Desktop\\Final year project\\fp\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:1862\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, hermitian)\u001b[39m\n\u001b[32m   1858\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->DdD\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->ddd\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1859\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_svd_nonconvergence,\n\u001b[32m   1860\u001b[39m               invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m, over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1861\u001b[39m               under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1862\u001b[39m     u, s, vh = \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1863\u001b[39m u = u.astype(result_t, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1864\u001b[39m s = s.astype(_realType(result_t), copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics.pairwise import linear_kernel, rbf_kernel, polynomial_kernel\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Define the BaseRVM class\n",
    "class BaseRVM(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel='rbf',\n",
    "        degree=3,\n",
    "        coef1=None,\n",
    "        coef0=0.0,\n",
    "        n_iter=3000,\n",
    "        tol=1e-3,\n",
    "        alpha=1e-6,\n",
    "        threshold_alpha=1e9,\n",
    "        beta=1e-6,\n",
    "        beta_fixed=False,\n",
    "        bias_used=True,\n",
    "        verbose=False\n",
    "    ):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.coef1 = coef1\n",
    "        self.coef0 = coef0\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.alpha = alpha\n",
    "        self.threshold_alpha = threshold_alpha\n",
    "        self.beta = beta\n",
    "        self.beta_fixed = beta_fixed\n",
    "        self.bias_used = bias_used\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'kernel': self.kernel,\n",
    "            'degree': self.degree,\n",
    "            'coef1': self.coef1,\n",
    "            'coef0': self.coef0,\n",
    "            'n_iter': self.n_iter,\n",
    "            'tol': self.tol,\n",
    "            'alpha': self.alpha,\n",
    "            'threshold_alpha': self.threshold_alpha,\n",
    "            'beta': self.beta,\n",
    "            'beta_fixed': self.beta_fixed,\n",
    "            'bias_used': self.bias_used,\n",
    "            'verbose': self.verbose\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def _apply_kernel(self, x, y):\n",
    "        if self.kernel == 'linear':\n",
    "            phi = linear_kernel(x, y)\n",
    "        elif self.kernel == 'rbf':\n",
    "            phi = rbf_kernel(x, y, self.coef1)\n",
    "        elif self.kernel == 'poly':\n",
    "            phi = polynomial_kernel(x, y, self.degree, self.coef1, self.coef0)\n",
    "        elif callable(self.kernel):\n",
    "            phi = self.kernel(x, y)\n",
    "            if len(phi.shape) != 2:\n",
    "                raise ValueError(\"Custom kernel function did not return 2D matrix\")\n",
    "            if phi.shape[0] != x.shape[0]:\n",
    "                raise ValueError(\"Custom kernel function did not return matrix with rows equal to number of data points\")\n",
    "        else:\n",
    "            raise ValueError(\"Kernel selection is invalid\")\n",
    "        if self.bias_used:\n",
    "            phi = np.append(phi, np.ones((phi.shape[0], 1)), axis=1)\n",
    "        return phi\n",
    "\n",
    "    def _prune(self):\n",
    "        keep_alpha = self.alpha_ < self.threshold_alpha\n",
    "        if not np.any(keep_alpha):\n",
    "            keep_alpha[0] = True\n",
    "            if self.bias_used:\n",
    "                keep_alpha[-1] = True\n",
    "        self.relevance_ = self.relevance_[keep_alpha[:-1]] if self.bias_used else self.relevance_[keep_alpha]\n",
    "        self.alpha_ = self.alpha_[keep_alpha]\n",
    "        self.alpha_old = self.alpha_old[keep_alpha]\n",
    "        self.gamma = self.gamma[keep_alpha]\n",
    "        self.phi = self.phi[:, keep_alpha]\n",
    "        self.sigma_ = self.sigma_[np.ix_(keep_alpha, keep_alpha)]\n",
    "        self.m_ = self.m_[keep_alpha]\n",
    "\n",
    "# Define the RVR class with numerical stability improvements\n",
    "class RVR(BaseRVM, RegressorMixin):\n",
    "    def _posterior(self):\n",
    "        i_s = np.diag(self.alpha_) + self.beta_ * np.dot(self.phi.T, self.phi)\n",
    "        self.sigma_ = np.linalg.pinv(i_s)  # Use pinv for numerical stability\n",
    "        self.m_ = self.beta_ * np.dot(self.sigma_, np.dot(self.phi.T, self.y))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.phi = self._apply_kernel(X, X)\n",
    "        n_basis_functions = self.phi.shape[1]\n",
    "        self.relevance_ = X\n",
    "        self.y = y\n",
    "        self.alpha_ = self.alpha * np.ones(n_basis_functions)\n",
    "        self.beta_ = self.beta\n",
    "        self.m_ = np.zeros(n_basis_functions)\n",
    "        self.alpha_old = self.alpha_\n",
    "        for i in range(self.n_iter):\n",
    "            self._posterior()\n",
    "            self.gamma = 1 - self.alpha_ * np.diag(self.sigma_)\n",
    "            self.alpha_ = self.gamma / (self.m_ ** 2 + 1e-10)  # Add epsilon to avoid division by zero\n",
    "            if not self.beta_fixed:\n",
    "                self.beta_ = (n_samples - np.sum(self.gamma)) / np.sum((y - np.dot(self.phi, self.m_)) ** 2)\n",
    "            self._prune()\n",
    "            if self.verbose:\n",
    "                print(f\"Iteration: {i}\")\n",
    "                print(f\"Alpha: {self.alpha_}\")\n",
    "                print(f\"Beta: {self.beta_}\")\n",
    "                print(f\"Gamma: {self.gamma}\")\n",
    "                print(f\"m: {self.m_}\")\n",
    "                print(f\"Relevance Vectors: {self.relevance_.shape[0]}\")\n",
    "            delta = np.amax(np.absolute(self.alpha_ - self.alpha_old))\n",
    "            if delta < self.tol and i > 1:\n",
    "                break\n",
    "            self.alpha_old = self.alpha_\n",
    "        if self.bias_used:\n",
    "            self.bias = self.m_[-1]\n",
    "        else:\n",
    "            self.bias = None\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, eval_MSE=False):\n",
    "        phi = self._apply_kernel(X, self.relevance_)\n",
    "        y = np.dot(phi, self.m_)\n",
    "        if eval_MSE:\n",
    "            MSE = (1 / self.beta_) + np.dot(phi, np.dot(self.sigma_, phi.T))\n",
    "            return y, MSE[:, 0]\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "# Define evaluation metrics\n",
    "def nse(observed, predicted):\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - np.mean(observed)) ** 2)\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def pcc(observed, predicted):\n",
    "    return np.corrcoef(observed, predicted)[0, 1]\n",
    "\n",
    "def mape(observed, predicted):\n",
    "    observed = np.where(observed == 0, 1e-10, observed)\n",
    "    return np.mean(np.abs((observed - predicted) / observed)) * 100\n",
    "\n",
    "def pbias(observed, predicted):\n",
    "    return np.sum(predicted - observed) / np.sum(observed) * 100\n",
    "\n",
    "# Custom NSE scorer for multi-output\n",
    "def nse_scorer(y_true, y_pred):\n",
    "    numerator = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    denominator = np.sum((y_true - np.mean(y_true, axis=0)) ** 2, axis=0)\n",
    "    nse = 1 - numerator / denominator\n",
    "    return np.mean(nse)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('synthetic_data_gaussian_copula.csv')\n",
    "\n",
    "# Define columns\n",
    "targets = ['Zn [ng/m**3]', 'As [ng/m**3]']\n",
    "drop_cols = ['No', 'Date/Time', 'Date/time end', 'Duration', 'Altitude [m]']\n",
    "categorical_features = ['Size fraction']\n",
    "numerical_features = [col for col in df.columns if col not in targets + drop_cols + categorical_features]\n",
    "\n",
    "# Set up the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MultiOutputRegressor(RVR(n_iter=100, verbose=False)))  # Reduced n_iter for speed\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'regressor__estimator__kernel': ['rbf', 'linear'],\n",
    "    'regressor__estimator__coef1': [0.1, 1.0, 10.0],  # Only affects RBF kernel\n",
    "    'regressor__estimator__alpha': [1e-6, 1e-4, 1e-2],\n",
    "    'regressor__estimator__beta': [1e-6, 1e-4, 1e-2]\n",
    "}\n",
    "\n",
    "# Sort data by time for TimeSeriesSplit\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "df = df.sort_values('Date/Time')\n",
    "\n",
    "# Prepare features and targets\n",
    "X = df.drop(columns=targets + drop_cols)\n",
    "y = df[targets]\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'neg_mean_absolute_error': 'neg_mean_absolute_error',\n",
    "    'neg_mean_squared_error': 'neg_mean_squared_error',\n",
    "    'r2': 'r2',\n",
    "    'nse': make_scorer(nse_scorer, greater_is_better=True)\n",
    "}\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "grid_search_kf = GridSearchCV(model, param_grid, cv=kf, scoring=scoring, refit='neg_mean_absolute_error')\n",
    "grid_search_kf.fit(X, y)\n",
    "\n",
    "# Get the best model and predictions for k-fold\n",
    "best_model_kf = grid_search_kf.best_estimator_\n",
    "y_pred_kf = cross_val_predict(best_model_kf, X, y, cv=kf)\n",
    "\n",
    "# Perform time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "grid_search_ts = GridSearchCV(model, param_grid, cv=tscv, scoring=scoring, refit='neg_mean_absolute_error')\n",
    "grid_search_ts.fit(X, y)\n",
    "\n",
    "# Get the best model and predictions for TimeSeriesSplit\n",
    "best_model_ts = grid_search_ts.best_estimator_\n",
    "y_pred_ts = np.full_like(y, np.nan)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    best_model_ts.fit(X_train, y_train)\n",
    "    y_pred_ts[test_index] = best_model_ts.predict(X_test)\n",
    "\n",
    "# Extract true and predicted values\n",
    "zn_true = df['Zn [ng/m**3]']\n",
    "as_true = df['As [ng/m**3]']\n",
    "zn_pred_kf = y_pred_kf[:, 0]\n",
    "as_pred_kf = y_pred_kf[:, 1]\n",
    "\n",
    "# Filter non-NaN predictions for TimeSeriesSplit\n",
    "mask = ~np.isnan(y_pred_ts[:, 0])\n",
    "zn_true_ts = zn_true[mask]\n",
    "as_true_ts = as_true[mask]\n",
    "zn_pred_ts = y_pred_ts[mask, 0]\n",
    "as_pred_ts = y_pred_ts[mask, 1]\n",
    "\n",
    "# Print results for k-fold cross-validation\n",
    "print(\"Standard k-Fold Cross-Validation (k=10)\")\n",
    "print(f\"Best parameters: {grid_search_kf.best_params_}\")\n",
    "print(f\"Best score: {grid_search_kf.best_score_:.4f} (neg_mean_absolute_error)\")\n",
    "print(\"CV results for other metrics:\")\n",
    "for metric in scoring:\n",
    "    mean_score = grid_search_kf.cv_results_[f'mean_test_{metric}'][grid_search_kf.best_index_]\n",
    "    std_score = grid_search_kf.cv_results_[f'std_test_{metric}'][grid_search_kf.best_index_]\n",
    "    print(f\"{metric}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation for Zn [ng/m³]:\")\n",
    "print(f\"MAE: {mean_absolute_error(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(zn_true, zn_pred_kf)):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"R2: {r2_score(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"NSE: {nse(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"PCC: {pcc(zn_true, zn_pred_kf):.4f}\")\n",
    "print(f\"MAPE: {mape(zn_true, zn_pred_kf):.4f}%\")\n",
    "print(f\"PBIAS: {pbias(zn_true, zn_pred_kf):.4f}%\")\n",
    "\n",
    "print(\"\\nEvaluation for As [ng/m³]:\")\n",
    "print(f\"MAE: {mean_absolute_error(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(as_true, as_pred_kf)):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"R2: {r2_score(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"NSE: {nse(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"PCC: {pcc(as_true, as_pred_kf):.4f}\")\n",
    "print(f\"MAPE: {mape(as_true, as_pred_kf):.4f}%\")\n",
    "print(f\"PBIAS: {pbias(as_true, as_pred_kf):.4f}%\")\n",
    "\n",
    "# Print results for time series cross-validation\n",
    "print(\"\\nTime Series Cross-Validation (k=5)\")\n",
    "print(f\"Best parameters: {grid_search_ts.best_params_}\")\n",
    "print(f\"Best score: {grid_search_ts.best_score_:.4f} (neg_mean_absolute_error)\")\n",
    "print(\"CV results for other metrics:\")\n",
    "for metric in scoring:\n",
    "    mean_score = grid_search_ts.cv_results_[f'mean_test_{metric}'][grid_search_ts.best_index_]\n",
    "    std_score = grid_search_ts.cv_results_[f'std_test_{metric}'][grid_search_ts.best_index_]\n",
    "    print(f\"{metric}: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluation for Zn [ng/m³] (TimeSeriesSplit, partial predictions):\")\n",
    "print(f\"MAE: {mean_absolute_error(zn_true_ts, zn_pred_ts):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(zn_true_ts, zn_pred_ts)):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(zn_true_ts, zn_pred_ts):.4f}\")\n",
    "print(f\"R2: {r2_score(zn_true_ts, zn_pred_ts):.4f}\")\n",
    "print(f\"NSE: {nse(zn_true_ts, zn_pred_ts):.4f}\")\n",
    "print(f\"PCC: {pcc(zn_true_ts, zn_pred_ts):.4f}\")\n",
    "print(f\"MAPE: {mape(zn_true_ts, zn_pred_ts):.4f}%\")\n",
    "print(f\"PBIAS: {pbias(zn_true_ts, zn_pred_ts):.4f}%\")\n",
    "\n",
    "print(\"\\nEvaluation for As [ng/m³] (TimeSeriesSplit, partial predictions):\")\n",
    "print(f\"MAE: {mean_absolute_error(as_true_ts, as_pred_ts):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(as_true_ts, as_pred_ts)):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(as_true_ts, as_pred_ts):.4f}\")\n",
    "print(f\"R2: {r2_score(as_true_ts, as_pred_ts):.4f}\")\n",
    "print(f\"NSE: {nse(as_true_ts, as_pred_ts):.4f}\")\n",
    "print(f\"PCC: {pcc(as_true_ts, as_pred_ts):.4f}\")\n",
    "print(f\"MAPE: {mape(as_true_ts, as_pred_ts):.4f}%\")\n",
    "print(f\"PBIAS: {pbias(as_true_ts, as_pred_ts):.4f}%\")\n",
    "print(f\"\\nNote: TimeSeriesSplit predictions cover {len(zn_true_ts)}/{len(zn_true)} samples due to non-partitioning nature of splits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
